{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "afb640dd-a1b4-4a6e-b5c3-01509ed30a42",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-22T03:09:16.335767Z",
     "iopub.status.busy": "2025-04-22T03:09:16.335380Z",
     "iopub.status.idle": "2025-04-22T03:09:18.631976Z",
     "shell.execute_reply": "2025-04-22T03:09:18.631059Z",
     "shell.execute_reply.started": "2025-04-22T03:09:16.335740Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9bff66d-6e03-4fbb-bda7-721cadcea495",
   "metadata": {},
   "source": [
    "## Using yolov7 to obtain keypoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7c02cfd3-6512-411e-8659-ae068c301804",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-22T03:09:23.362990Z",
     "iopub.status.busy": "2025-04-22T03:09:23.362526Z",
     "iopub.status.idle": "2025-04-22T03:09:24.964694Z",
     "shell.execute_reply": "2025-04-22T03:09:24.963311Z",
     "shell.execute_reply.started": "2025-04-22T03:09:23.362953Z"
    }
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import time\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "from torchvision import transforms\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1d69a61d-dd18-4430-9dae-b6548baf6a22",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-22T03:09:28.802779Z",
     "iopub.status.busy": "2025-04-22T03:09:28.802599Z",
     "iopub.status.idle": "2025-04-22T03:09:28.815740Z",
     "shell.execute_reply": "2025-04-22T03:09:28.815740Z",
     "shell.execute_reply.started": "2025-04-22T03:09:28.802752Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/notebooks/Fighting-detection-in-CCTVs/yolov7\n"
     ]
    }
   ],
   "source": [
    "# Change directory to \"/yolov7\"\n",
    "os.chdir(\"yolov7\")\n",
    "print(os.getcwd()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3e2e3cbe-0a60-4f82-a842-72ecb1286c6c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-22T03:09:36.944894Z",
     "iopub.status.busy": "2025-04-22T03:09:36.944894Z",
     "iopub.status.idle": "2025-04-22T03:09:38.723697Z",
     "shell.execute_reply": "2025-04-22T03:09:38.723697Z",
     "shell.execute_reply.started": "2025-04-22T03:09:36.944894Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model:  yolov7-pose...\n",
      "Model loaded.\n"
     ]
    }
   ],
   "source": [
    "from utils.datasets import letterbox\n",
    "from utils.general import non_max_suppression_kpt\n",
    "from utils.plots import output_to_keypoint, plot_skeleton_kpts\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "###############################################################################\n",
    "\n",
    "# Load yolov7 pose detector model\n",
    "print(\"Loading model: \", \"yolov7-pose...\")\n",
    "weights_path = \"yolov7-w6-pose.pt\"\n",
    "model_yolov7 = torch.load(weights_path, map_location=device, weights_only=False)['model']\n",
    "print(\"Model loaded.\")\n",
    "\n",
    "model_yolov7.float().eval()\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    # half() turns predictions into float16 tensors --> significantly lowers inference time\n",
    "    model_yolov7.half().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2025997a-0e94-4581-86de-fc7a27c9ca09",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-22T03:09:42.773374Z",
     "iopub.status.busy": "2025-04-22T03:09:42.772784Z",
     "iopub.status.idle": "2025-04-22T03:09:42.786972Z",
     "shell.execute_reply": "2025-04-22T03:09:42.785546Z",
     "shell.execute_reply.started": "2025-04-22T03:09:42.773182Z"
    }
   },
   "outputs": [],
   "source": [
    "# Main inference\n",
    "def infer(image):\n",
    "    image = letterbox(image, 960, \n",
    "                      stride=64,\n",
    "                      auto=True)[0]  # shape: (567, 960, 3)\n",
    "    \n",
    "    image = transforms.ToTensor()(image)  # torch.Size([3, 567, 960])\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        image = image.half().to(device)\n",
    "\n",
    "    image = image.unsqueeze(0)  # torch.Size([1, 3, 567, 960])\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output, _ = model_yolov7(image)\n",
    "\n",
    "    return output, image\n",
    "\n",
    "###############################################################################\n",
    "# Draw YOLOv7 pose keypoints and optionally return keypoints for saving.\n",
    "def draw_keypoints(output, image, confidence=0.25, threshold=0.65, return_kpts=False, background_colour=(255, 255, 255)):\n",
    "    output = non_max_suppression_kpt(\n",
    "        output,\n",
    "        confidence,\n",
    "        threshold,\n",
    "        nc=model_yolov7.yaml['nc'],\n",
    "        nkpt=model_yolov7.yaml['nkpt'],\n",
    "        kpt_label=True)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = output_to_keypoint(output)  # shape: (num_people, 51)\n",
    "\n",
    "    nimg = image[0].permute(1, 2, 0) * 255\n",
    "    nimg = cv2.cvtColor(nimg.cpu().numpy().astype(np.uint8), cv2.COLOR_RGB2BGR)\n",
    "    \n",
    "    # Create a blank image with the specified background colour\n",
    "    # nimg = np.full((image.shape[2], image.shape[3], 3), background_colour, dtype=np.uint8)\n",
    "\n",
    "    # Correctly loop through 'output' variable instead of 'kpts'\n",
    "    for idx in range(output.shape[0]):\n",
    "        plot_skeleton_kpts(nimg, output[idx, 7:].T, 3)\n",
    "\n",
    "    if return_kpts:\n",
    "        return nimg, output  # (image with keypoints drawn, raw keypoints)\n",
    "\n",
    "    return nimg\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1cc098d-f0b0-4235-bc3c-c5c6f39cdefa",
   "metadata": {},
   "source": [
    "## Create custom dataloader\n",
    "- includes pre-processing via yolov7 pose estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "35dd92b9-4288-4bf1-b64a-15d078deaa26",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-22T03:18:10.187557Z",
     "iopub.status.busy": "2025-04-22T03:18:10.187198Z",
     "iopub.status.idle": "2025-04-22T03:18:10.207369Z",
     "shell.execute_reply": "2025-04-22T03:18:10.205903Z",
     "shell.execute_reply.started": "2025-04-22T03:18:10.187533Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "transform = transforms.Compose([\n",
    "    # Convert frames to PyTorch tensors\n",
    "    transforms.ToTensor(), \n",
    "    # Resize all video frames to 224x224\n",
    "    transforms.Resize((224,224)), \n",
    "])\n",
    "\n",
    "class VideoDataset(Dataset):\n",
    "    def __init__(self, video_files, video_folder, transform=transform):\n",
    "        self.video_folder = video_folder\n",
    "        self.video_files = video_files\n",
    "        self.transform = transform\n",
    "        self.data = self._load_data()\n",
    "    \n",
    "    def _load_data(self):\n",
    "        data = []\n",
    "        count = 5\n",
    "        \n",
    "        for video_file in self.video_files:\n",
    "            video_path = os.path.join(self.video_folder, video_file)\n",
    "            # if the video name is \"nofixxx.mp4\", this means no-fight --> 0\n",
    "            # if \"NV_xx.mp4\" means no-fight\n",
    "            if \"nofi\" in video_file or \"NV\" in video_file:\n",
    "                label = 0\n",
    "            else:\n",
    "                label = 1\n",
    "                \n",
    "            # Check if we extract the labels correctly\n",
    "            # if count >= 0:\n",
    "            #     print(f'video path: {video_path}')\n",
    "            #     print(f'label: {label}')\n",
    "            #     count -= 1\n",
    "                                      \n",
    "            capture = cv2.VideoCapture(video_path)\n",
    "            fps = int(capture.get(cv2.CAP_PROP_FPS))\n",
    "            total_frames = int(capture.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "             \n",
    "            for idx in range(0, total_frames, fps): # sample one frame per second\n",
    "                try:\n",
    "                    capture.set(cv2.CAP_PROP_POS_FRAMES, idx)\n",
    "                    ret, frame = capture.read()\n",
    "                    if ret:\n",
    "                        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                        keypoints, image = infer(frame_rgb)\n",
    "                        pose_image = draw_keypoints(keypoints, image, return_kpts=False)\n",
    "\n",
    "                        # Check the outputs given by yolov7 output\n",
    "                        # if count >= 0:\n",
    "                        #     plt.figure(figsize=(30, 7))\n",
    "                        #     plt.axis(\"off\")\n",
    "                        #     plt.imshow(pose_image)\n",
    "                        #     plt.savefig(f\"output{count}.jpg\") # Save output if needed\n",
    "\n",
    "                        data.append((pose_image, label))\n",
    "                        \n",
    "                # If there are errors, skip it\n",
    "                except cv2.error as e:\n",
    "                    print(f\"Error processing frame: {e}\")\n",
    "                    continue\n",
    "            capture.release()\n",
    "        return data\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        pose_image, label = self.data[idx]\n",
    "        if self.transform:\n",
    "            pose_image = self.transform(pose_image)\n",
    "        return pose_image, label\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df74bf1a-58ab-4de4-a2eb-b9a72fcf9be6",
   "metadata": {},
   "source": [
    "## Prepare video paths and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9d1ca897-2108-45c1-891a-77c7abfd28b4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-17T09:29:40.436475Z",
     "iopub.status.busy": "2025-04-17T09:29:40.431946Z",
     "iopub.status.idle": "2025-04-17T09:31:52.541710Z",
     "shell.execute_reply": "2025-04-17T09:31:52.540646Z",
     "shell.execute_reply.started": "2025-04-17T09:29:40.436475Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing dataloaders...\n",
      "Splitting data...\n",
      "Train dataset compiled...\n",
      "Validation dataset compiled...\n",
      "Test dataset compiled...\n",
      "Train dataloader compiled...\n",
      "Validation dataloader compiled...\n",
      "Test dataloader compiled...\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "print(\"Preparing dataloaders...\")\n",
    "\n",
    "video_folder = \"../fight-detection-3\"\n",
    "video_files = [f for f in os.listdir(video_folder) if f.endswith((\".mp4\", \".avi\", \".mov\", \".mpeg\"))]\n",
    "\n",
    "###############################################################################\n",
    "\n",
    "print(\"Splitting data...\")\n",
    "# Split dataset such that 20% of data is used for testing, remaining 80% for training + validation\n",
    "train_files, test_files = train_test_split(video_files, test_size=0.2, random_state=42)\n",
    "# Split dataset such that 60% of data is used for training, remaining 20% for validation\n",
    "train_files, val_files = train_test_split(train_files, test_size=0.25, random_state=42) # 0.25 * 0.8 = 0.2\n",
    "\n",
    "###############################################################################\n",
    "\n",
    "train_dataset = VideoDataset(train_files, video_folder)\n",
    "print(\"Train dataset compiled...\")\n",
    "val_dataset = VideoDataset(val_files, video_folder)\n",
    "print(\"Validation dataset compiled...\")\n",
    "test_dataset = VideoDataset(test_files, video_folder)\n",
    "print(\"Test dataset compiled...\")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "print(\"Train dataloader compiled...\")\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "print(\"Validation dataloader compiled...\")\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "print(\"Test dataloader compiled...\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a75ac1a-48f0-444d-a802-8e36db6c1117",
   "metadata": {},
   "source": [
    "## Create pytorch instance of MobileNetV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5585e978-e1a1-4a5b-8497-9aea8434c7ab",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-22T03:09:57.151513Z",
     "iopub.status.busy": "2025-04-22T03:09:57.151093Z",
     "iopub.status.idle": "2025-04-22T03:09:57.551320Z",
     "shell.execute_reply": "2025-04-22T03:09:57.551320Z",
     "shell.execute_reply.started": "2025-04-22T03:09:57.151513Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MobileNet_V2_Weights.IMAGENET1K_V1`. You can also use `weights=MobileNet_V2_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/mobilenet_v2-b0353104.pth\" to /root/.cache/torch/hub/checkpoints/mobilenet_v2-b0353104.pth\n",
      "100%|██████████| 13.6M/13.6M [00:00<00:00, 107MB/s] \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MobileNetV2(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2dNormActivation(\n",
       "      (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU6(inplace=True)\n",
       "    )\n",
       "    (1): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (2): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)\n",
       "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (3): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n",
       "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (4): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)\n",
       "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (5): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
       "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (6): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
       "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (7): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)\n",
       "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (8): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (9): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (10): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (11): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (12): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n",
       "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (13): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n",
       "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (14): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=576, bias=False)\n",
       "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(576, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (15): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
       "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (16): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
       "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (17): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
       "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(960, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (18): Conv2dNormActivation(\n",
       "      (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU6(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): Dropout(p=0.2, inplace=False)\n",
       "    (1): Linear(in_features=1280, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "# Obtain a pre-trained mobilenet_v2\n",
    "model_mobilenet = models.mobilenet_v2(pretrained=True)\n",
    "\n",
    "# Modify the final layer into a binary classification  \n",
    "# --> we apply sigmoid activation during the forward pass or within the loss function\n",
    "model_mobilenet.classifier[1] = nn.Linear(model_mobilenet.classifier[1].in_features, 1)\n",
    "\n",
    "# Freeze the base model and only train the classifier layer\n",
    "for param in model_mobilenet.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "for param in model_mobilenet.classifier.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "\n",
    "model_mobilenet.to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4623e85f-470c-43f5-aae0-e1079437b7fb",
   "metadata": {},
   "source": [
    "## Train MobileNet on fighting and non-fighting datasets\n",
    "- lr = 0.001\n",
    "- epochs = 100\n",
    "- Adam optimiser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f3a7afdb-ad01-4b67-aa91-6ee2ff209263",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-22T03:10:02.493611Z",
     "iopub.status.busy": "2025-04-22T03:10:02.493207Z",
     "iopub.status.idle": "2025-04-22T03:10:02.693268Z",
     "shell.execute_reply": "2025-04-22T03:10:02.692248Z",
     "shell.execute_reply.started": "2025-04-22T03:10:02.493572Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the weights\n",
    "model_mobilenet.load_state_dict(torch.load('../model_epoch_recent_0001.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd9fd588-49e3-4498-9588-8451a3afe2db",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-17T09:31:52.731765Z",
     "iopub.status.busy": "2025-04-17T09:31:52.731525Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted value: tensor([[1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.]], grad_fn=<ToCopyBackward0>)\n",
      "Label: tensor([[1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.]])\n",
      "Predicted value: tensor([[0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.]], grad_fn=<ToCopyBackward0>)\n",
      "Label: tensor([[0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]])\n",
      "Epoch [1/100], Loss: 1.0597\n",
      "Epoch [2/100], Loss: 0.6255\n",
      "Epoch [3/100], Loss: 0.2197\n",
      "Epoch [4/100], Loss: 0.1516\n",
      "Epoch [5/100], Loss: 0.3519\n",
      "Epoch [6/100], Loss: 0.3776\n",
      "Epoch [7/100], Loss: 0.4070\n",
      "Epoch [8/100], Loss: 0.4276\n",
      "Epoch [9/100], Loss: 0.4745\n",
      "Epoch [10/100], Loss: 0.3810\n",
      "Epoch [11/100], Loss: 0.4217\n",
      "Epoch [12/100], Loss: 0.3548\n",
      "Epoch [13/100], Loss: 0.2543\n",
      "Epoch [14/100], Loss: 0.5215\n",
      "Epoch [15/100], Loss: 0.4538\n",
      "Epoch [16/100], Loss: 0.4985\n",
      "Epoch [17/100], Loss: 0.6122\n",
      "Epoch [18/100], Loss: 0.6021\n",
      "Epoch [19/100], Loss: 0.6146\n",
      "Epoch [20/100], Loss: 0.2062\n",
      "Epoch [21/100], Loss: 0.7915\n",
      "Epoch [22/100], Loss: 0.4705\n",
      "Epoch [23/100], Loss: 0.2937\n",
      "Epoch [24/100], Loss: 0.1693\n",
      "Epoch [25/100], Loss: 0.2319\n",
      "Epoch [26/100], Loss: 0.7569\n",
      "Epoch [27/100], Loss: 0.4143\n",
      "Epoch [28/100], Loss: 0.1312\n",
      "Epoch [29/100], Loss: 0.7000\n",
      "Epoch [30/100], Loss: 0.3043\n",
      "Epoch [31/100], Loss: 0.5893\n",
      "Epoch [32/100], Loss: 0.2894\n",
      "Epoch [33/100], Loss: 0.2501\n",
      "Epoch [34/100], Loss: 0.2817\n",
      "Epoch [35/100], Loss: 0.4468\n",
      "Epoch [36/100], Loss: 0.1556\n",
      "Epoch [37/100], Loss: 0.2640\n",
      "Epoch [38/100], Loss: 0.3193\n",
      "Epoch [39/100], Loss: 0.2242\n",
      "Epoch [40/100], Loss: 0.5010\n",
      "Epoch [41/100], Loss: 0.3096\n",
      "Epoch [42/100], Loss: 0.2226\n",
      "Epoch [43/100], Loss: 0.2018\n",
      "Epoch [44/100], Loss: 0.4534\n",
      "Epoch [45/100], Loss: 0.8031\n",
      "Epoch [46/100], Loss: 0.4743\n",
      "Epoch [47/100], Loss: 0.3125\n",
      "Epoch [48/100], Loss: 0.2984\n",
      "Epoch [49/100], Loss: 0.3729\n",
      "Epoch [50/100], Loss: 0.2464\n",
      "Epoch [51/100], Loss: 0.3936\n",
      "Epoch [52/100], Loss: 0.4168\n",
      "Epoch [53/100], Loss: 0.1591\n",
      "Epoch [54/100], Loss: 0.2060\n",
      "Epoch [55/100], Loss: 0.3585\n",
      "Epoch [56/100], Loss: 0.2521\n",
      "Epoch [57/100], Loss: 0.5044\n",
      "Epoch [58/100], Loss: 0.2174\n",
      "Epoch [59/100], Loss: 0.1696\n",
      "Epoch [60/100], Loss: 0.1151\n",
      "Epoch [61/100], Loss: 0.6619\n",
      "Epoch [62/100], Loss: 0.4177\n",
      "Epoch [63/100], Loss: 0.3097\n",
      "Epoch [64/100], Loss: 0.3181\n",
      "Epoch [65/100], Loss: 0.2256\n",
      "Epoch [66/100], Loss: 0.3516\n",
      "Epoch [67/100], Loss: 0.4377\n",
      "Epoch [68/100], Loss: 0.3281\n",
      "Epoch [69/100], Loss: 0.5328\n",
      "Epoch [70/100], Loss: 0.3004\n",
      "Epoch [71/100], Loss: 0.3151\n",
      "Epoch [72/100], Loss: 0.3452\n",
      "Epoch [73/100], Loss: 0.3394\n",
      "Epoch [74/100], Loss: 0.4876\n",
      "Epoch [75/100], Loss: 0.1363\n",
      "Epoch [76/100], Loss: 0.4113\n",
      "Epoch [77/100], Loss: 0.6419\n",
      "Epoch [78/100], Loss: 0.4442\n",
      "Epoch [79/100], Loss: 0.4088\n",
      "Epoch [80/100], Loss: 0.3652\n",
      "Epoch [81/100], Loss: 0.4636\n",
      "Epoch [82/100], Loss: 0.1043\n",
      "Epoch [83/100], Loss: 0.1105\n",
      "Epoch [84/100], Loss: 0.1416\n",
      "Epoch [85/100], Loss: 0.2413\n",
      "Epoch [86/100], Loss: 0.3906\n",
      "Epoch [87/100], Loss: 0.3343\n",
      "Epoch [88/100], Loss: 0.0762\n",
      "Epoch [89/100], Loss: 0.5649\n"
     ]
    }
   ],
   "source": [
    "def validate(model, val_loader):\n",
    "    # Set the model to evaluation mode\n",
    "    model.eval()  \n",
    "    val_loss = 0.0\n",
    "    correct, total = 0, 0\n",
    "    # Ensure the same loss function is used\n",
    "    criterion = nn.BCEWithLogitsLoss()  \n",
    "\n",
    "    # Disable gradient computation\n",
    "    with torch.no_grad():  \n",
    "        for pose_img, labels in val_loader:\n",
    "            pose_img, labels = pose_img.to(device), labels.to(device).float()\n",
    "            outputs = model(pose_img)\n",
    "            \n",
    "            # Accumulate the loss\n",
    "            loss = criterion(outputs, labels.unsqueeze(1))\n",
    "            val_loss += loss.item() * pose_img.size(0)  \n",
    "            \n",
    "            # Calculate accuracy\n",
    "            predicted = torch.round(torch.sigmoid(outputs))\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted.cpu() == labels.cpu().unsqueeze(1)).sum().item()\n",
    "\n",
    "    # Compute the average loss\n",
    "    val_loss /= len(val_loader.dataset)  \n",
    "    val_accuracy = correct / total\n",
    "    return val_loss, val_accuracy\n",
    "\n",
    "###############################################################################\n",
    "\n",
    "num_epochs = 100\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "train_accuracies = []\n",
    "val_accuracies = []\n",
    "\n",
    "# Define loss function and optimiser\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model_mobilenet.classifier.parameters(), lr=0.0001)\n",
    "\n",
    "count = 1\n",
    "\n",
    "###############################################################################\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model_mobilenet.train()\n",
    "    \n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for pose_img, labels in train_loader:\n",
    "        inputs, labels = pose_img.to(device), labels.to(device).float()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model_mobilenet(inputs)\n",
    "        loss = criterion(outputs, labels.unsqueeze(1).float())\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        predicted = torch.round(torch.sigmoid(outputs))\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted.cpu() == labels.cpu().unsqueeze(1)).sum().item()\n",
    "        \n",
    "        # For debugging - checking the contrast between predicted values and true labels\n",
    "        if count >= 0:\n",
    "            print(f\"Predicted value: {predicted.cpu()}\")\n",
    "            print(f\"Label: {labels.cpu().unsqueeze(1)}\")\n",
    "            count -= 1\n",
    "\n",
    "    epoch_loss = running_loss / len(train_loader.dataset)\n",
    "    epoch_accuracy = correct / total\n",
    "    train_losses.append(epoch_loss)\n",
    "    train_accuracies.append(epoch_accuracy)\n",
    "\n",
    "    # Validation step\n",
    "    val_loss, val_accuracy = validate(model_mobilenet, val_loader)\n",
    "    val_losses.append(val_loss)\n",
    "    val_accuracies.append(val_accuracy)\n",
    "    \n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "    \n",
    "    # Save model weights for every 5 epochs\n",
    "    # if epoch % 5 == 0:\n",
    "    #     torch.save(model_mobilenet.state_dict(), f'../model_epoch_2_{epoch+1}.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7071fa9d-3eb1-495a-bd5f-4de5255f1353",
   "metadata": {},
   "source": [
    "## Save model weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca2eb716-2a4b-4035-b250-ba1748410436",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model_mobilenet.state_dict(), f'../model_epoch_recent_0001.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1332c65f-95d9-432b-bfd0-d3a3cad2e2a2",
   "metadata": {},
   "source": [
    "## Show training-val loss and accuracy over epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11da6e3a-99bf-4c27-9c20-822523b09c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# Plot training and validation loss\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(range(1, num_epochs+1), train_losses, label='Training Loss')\n",
    "plt.plot(range(1, num_epochs+1), val_losses, label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.show()\n",
    "\n",
    "# Plot training and validation accuracy\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(range(1, num_epochs+1), train_accuracies, label='Training Accuracy')\n",
    "plt.plot(range(1, num_epochs+1), val_accuracies, label='Validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e43bb2-251a-4ed0-9035-768b67245e7e",
   "metadata": {},
   "source": [
    "## Testloader\n",
    "- can be used for unseen dataset\n",
    "- or for the 20% split dataset done earlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "15429093-c2ec-460d-9548-4502cec5d94c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-22T03:18:17.820478Z",
     "iopub.status.busy": "2025-04-22T03:18:17.819081Z",
     "iopub.status.idle": "2025-04-22T03:23:59.165407Z",
     "shell.execute_reply": "2025-04-22T03:23:59.163923Z",
     "shell.execute_reply.started": "2025-04-22T03:18:17.820433Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing test dataloader...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[h264 @ 0x5b276280] mb_type 104 in P slice too large at 98 31\n",
      "[h264 @ 0x5b276280] error while decoding MB 98 31\n",
      "[h264 @ 0x5b276280] mb_type 104 in P slice too large at 98 31\n",
      "[h264 @ 0x5b276280] error while decoding MB 98 31\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test dataset compiled...\n",
      "Test dataloader compiled...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.7426505282063534 Test accuracy: 0.606655290102389\n"
     ]
    }
   ],
   "source": [
    "# Load weights\n",
    "model_mobilenet.load_state_dict(torch.load('../model_epoch_recent_0001.pth'))\n",
    "\n",
    "###############################################################################\n",
    "\n",
    "# Use the Dataloader for unseen dataset\n",
    "print(\"Preparing test dataloader...\")\n",
    "\n",
    "video_folder = \"../fight-detection-5\"\n",
    "video_files = [f for f in os.listdir(video_folder) if f.endswith((\".mp4\", \".avi\", \".mov\", \".mpeg\"))]\n",
    "\n",
    "test_dataset = VideoDataset(video_files, video_folder)\n",
    "print(\"Test dataset compiled...\")\n",
    "\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "print(\"Test dataloader compiled...\")\n",
    "\n",
    "###############################################################################\n",
    "\n",
    "# Set up the testing function\n",
    "def test(model, test_loader=test_loader):\n",
    "    # Set the model to evaluation mode\n",
    "    model.eval()  \n",
    "    test_loss = 0.0\n",
    "    correct, total = 0, 0\n",
    "    \n",
    "    # Ensure the same loss function is used\n",
    "    criterion = nn.BCEWithLogitsLoss()  \n",
    "\n",
    "    # Disable gradient computation\n",
    "    with torch.no_grad():  \n",
    "        for keypoints, labels in test_loader:\n",
    "            keypoints, labels = keypoints.to(device), labels.to(device).float()\n",
    "            # Model inference\n",
    "            outputs = model(keypoints)\n",
    "            predicted = torch.round(torch.sigmoid(outputs))\n",
    "            # Chose this if custom threshold is needed\n",
    "            # threshold = 0.3\n",
    "            # predicted = (outputs > threshold).float()\n",
    "\n",
    "            # Accumulate the loss\n",
    "            loss = criterion(outputs, labels.unsqueeze(1))\n",
    "            test_loss += loss.item() * keypoints.size(0)  \n",
    "            \n",
    "            # Calculate accuracy\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted.cpu() == labels.cpu().unsqueeze(1)).sum().item()\n",
    "\n",
    "    # Compute the average loss\n",
    "    test_loss /= len(test_loader.dataset)  \n",
    "    test_accuracy = correct / total\n",
    "    return test_loss, test_accuracy\n",
    "\n",
    "###############################################################################\n",
    "\n",
    "# Call the test function and test it via the the test_loader\n",
    "test_loss, test_accuracy = test(model_mobilenet)\n",
    "print(f\"Test loss: {test_loss} Test accuracy: {test_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6247b596-6e48-4555-a732-4b0ca2180bb9",
   "metadata": {},
   "source": [
    "## Basic inference and testing - for one video file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3a136c1d-028f-4f65-aa54-8128e30f6e26",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-22T03:31:18.622293Z",
     "iopub.status.busy": "2025-04-22T03:31:18.621867Z",
     "iopub.status.idle": "2025-04-22T03:31:20.242611Z",
     "shell.execute_reply": "2025-04-22T03:31:20.241302Z",
     "shell.execute_reply.started": "2025-04-22T03:31:18.622284Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video compiled...\n",
      "Video loaded, proceeding to inferencing.\n",
      "Predicted result for batch: tensor([[1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.]], device='cuda:0', grad_fn=<RoundBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Load weights\n",
    "model_mobilenet.load_state_dict(torch.load('../model_epoch_recent_0001.pth'))\n",
    "model_mobilenet.eval()\n",
    "\n",
    "###############################################################################\n",
    "\n",
    "# Pre-process input --> extract frames\n",
    "video_path = '../fight-detection-2/V_96.mp4'\n",
    "# video_path = '../fight-0002.mpeg' # *mpeg videos cannot be processed\n",
    "\n",
    "###############################################################################\n",
    "\n",
    "class Video(Dataset):\n",
    "    def __init__(self, video_path, transform=transform):\n",
    "        self.video_path = video_path\n",
    "        self.transform = transform\n",
    "        self.data = self._load_data()\n",
    "\n",
    "    def _load_data(self):\n",
    "        data = []\n",
    "                                      \n",
    "        capture = cv2.VideoCapture(self.video_path)\n",
    "        fps = int(capture.get(cv2.CAP_PROP_FPS))\n",
    "        total_frames = int(capture.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "        for idx in range(0, total_frames, fps): # Sample one frame per second\n",
    "            try:\n",
    "                capture.set(cv2.CAP_PROP_POS_FRAMES, idx)\n",
    "                ret, frame = capture.read()\n",
    "                if ret:\n",
    "                    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                    keypoints, image = infer(frame_rgb)\n",
    "                    pose_image = draw_keypoints(keypoints, image, return_kpts=False)\n",
    "                    data.append(pose_image)\n",
    "                    \n",
    "            # If there are errors, skip it\n",
    "            except cv2.error as e:\n",
    "                print(f\"Error processing frame: {e}\")\n",
    "                continue\n",
    "                \n",
    "        capture.release()\n",
    "        return data\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        pose_image = self.data[idx]\n",
    "        if self.transform:\n",
    "            pose_image = self.transform(pose_image)\n",
    "        return pose_image\n",
    "\n",
    "###############################################################################\n",
    "\n",
    "processed_video = Video(video_path)\n",
    "print(\"Video compiled...\")\n",
    "\n",
    "num_processed_frames = len(processed_video)\n",
    "if num_processed_frames == 0:\n",
    "    print(\"Video is too short to process or there is not enough footage with a person in it.\")\n",
    "    print(\"Video resolution could also be poor that no pose is estimated :(\")\n",
    "\n",
    "else:\n",
    "    video_loader = DataLoader(processed_video, batch_size=32, shuffle=False)     \n",
    "    print(\"Video loaded, proceeding to inferencing.\")\n",
    "     \n",
    "    ###############################################################################\n",
    "\n",
    "    # Inference\n",
    "    for pose_img in video_loader:\n",
    "        pose_img = pose_img.to(device)\n",
    "        outputs = model_mobilenet(pose_img)\n",
    "        predicted = torch.round(torch.sigmoid(outputs))\n",
    "        print(f\"Predicted result for batch: {predicted}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "471f69ef-fe07-444c-b12c-d487aab7c782",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
