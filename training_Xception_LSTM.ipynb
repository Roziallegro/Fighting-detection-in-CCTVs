{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "afb640dd-a1b4-4a6e-b5c3-01509ed30a42",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-19T08:51:03.743902Z",
     "iopub.status.busy": "2025-04-19T08:51:03.743031Z",
     "iopub.status.idle": "2025-04-19T08:51:05.715628Z",
     "shell.execute_reply": "2025-04-19T08:51:05.710504Z",
     "shell.execute_reply.started": "2025-04-19T08:51:03.743848Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9bff66d-6e03-4fbb-bda7-721cadcea495",
   "metadata": {},
   "source": [
    "## Using yolov7 to obtain keypoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7c02cfd3-6512-411e-8659-ae068c301804",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-19T07:43:44.411075Z",
     "iopub.status.busy": "2025-04-19T07:43:44.410180Z",
     "iopub.status.idle": "2025-04-19T07:43:47.090168Z",
     "shell.execute_reply": "2025-04-19T07:43:47.089236Z",
     "shell.execute_reply.started": "2025-04-19T07:43:44.411033Z"
    }
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import time\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "from torchvision import transforms\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1d69a61d-dd18-4430-9dae-b6548baf6a22",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-19T07:43:49.271303Z",
     "iopub.status.busy": "2025-04-19T07:43:49.267711Z",
     "iopub.status.idle": "2025-04-19T07:43:49.275310Z",
     "shell.execute_reply": "2025-04-19T07:43:49.275310Z",
     "shell.execute_reply.started": "2025-04-19T07:43:49.271303Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/notebooks/Fighting-detection-in-CCTVs/yolov7\n"
     ]
    }
   ],
   "source": [
    "# Change directory to \"/yolov7\"\n",
    "os.chdir(\"yolov7\")\n",
    "print(os.getcwd()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3e2e3cbe-0a60-4f82-a842-72ecb1286c6c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-19T07:43:51.089809Z",
     "iopub.status.busy": "2025-04-19T07:43:51.089456Z",
     "iopub.status.idle": "2025-04-19T07:43:55.701034Z",
     "shell.execute_reply": "2025-04-19T07:43:55.699743Z",
     "shell.execute_reply.started": "2025-04-19T07:43:51.089783Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model:  yolov7-pose...\n",
      "Model loaded.\n"
     ]
    }
   ],
   "source": [
    "from utils.datasets import letterbox\n",
    "from utils.general import non_max_suppression_kpt\n",
    "from utils.plots import output_to_keypoint, plot_skeleton_kpts\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "###############################################################################\n",
    "\n",
    "# Load yolov7 pose detector model\n",
    "print(\"Loading model: \", \"yolov7-pose...\")\n",
    "weights_path = \"yolov7-w6-pose.pt\"\n",
    "model_yolov7 = torch.load(weights_path, map_location=device, weights_only=False)['model']\n",
    "print(\"Model loaded.\")\n",
    "\n",
    "model_yolov7.float().eval()\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    # half() turns predictions into float16 tensors --> significantly lowers inference time\n",
    "    model_yolov7.half().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2025997a-0e94-4581-86de-fc7a27c9ca09",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-19T03:55:06.455635Z",
     "iopub.status.busy": "2025-04-19T03:55:06.455121Z",
     "iopub.status.idle": "2025-04-19T03:55:06.468169Z",
     "shell.execute_reply": "2025-04-19T03:55:06.462562Z",
     "shell.execute_reply.started": "2025-04-19T03:55:06.455609Z"
    }
   },
   "outputs": [],
   "source": [
    "# Main inference\n",
    "def infer(image):\n",
    "    image = letterbox(image, 960, \n",
    "                      stride=64,\n",
    "                      auto=True)[0]  # shape: (567, 960, 3)\n",
    "    \n",
    "    image = transforms.ToTensor()(image)  # torch.Size([3, 567, 960])\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        image = image.half().to(device)\n",
    "\n",
    "    image = image.unsqueeze(0)  # torch.Size([1, 3, 567, 960])\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output, _ = model_yolov7(image)\n",
    "\n",
    "    return output, image\n",
    "\n",
    "###############################################################################\n",
    "# Draw YOLOv7 pose keypoints and optionally return keypoints for saving.\n",
    "def draw_keypoints(output, image, confidence=0.25, threshold=0.65, return_kpts=False, background_colour=(255, 255, 255)):\n",
    "    output = non_max_suppression_kpt(\n",
    "        output,\n",
    "        confidence,\n",
    "        threshold,\n",
    "        nc=model_yolov7.yaml['nc'],\n",
    "        nkpt=model_yolov7.yaml['nkpt'],\n",
    "        kpt_label=True)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = output_to_keypoint(output)  # shape: (num_people, 51)\n",
    "\n",
    "    nimg = image[0].permute(1, 2, 0) * 255\n",
    "    nimg = cv2.cvtColor(nimg.cpu().numpy().astype(np.uint8), cv2.COLOR_RGB2BGR)\n",
    "    \n",
    "    # Create a blank image with the specified background colour\n",
    "    # nimg = np.full((image.shape[2], image.shape[3], 3), background_colour, dtype=np.uint8)\n",
    "\n",
    "    # Correctly loop through 'output' variable instead of 'kpts'\n",
    "    for idx in range(output.shape[0]):\n",
    "        plot_skeleton_kpts(nimg, output[idx, 7:].T, 3)\n",
    "\n",
    "    if return_kpts:\n",
    "        return nimg, output  # (image with keypoints drawn, raw keypoints)\n",
    "\n",
    "    return nimg\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1cc098d-f0b0-4235-bc3c-c5c6f39cdefa",
   "metadata": {},
   "source": [
    "## Create custom dataloader\n",
    "- includes pre-processing via yolov7 pose estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "35dd92b9-4288-4bf1-b64a-15d078deaa26",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-19T04:36:39.706802Z",
     "iopub.status.busy": "2025-04-19T04:36:39.706241Z",
     "iopub.status.idle": "2025-04-19T04:36:39.717379Z",
     "shell.execute_reply": "2025-04-19T04:36:39.711946Z",
     "shell.execute_reply.started": "2025-04-19T04:36:39.706785Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "transform = transforms.Compose([\n",
    "    # Convert frames to PyTorch tensors\n",
    "    transforms.ToTensor(), \n",
    "    # Resize all video frames to 480, 640 (480p)\n",
    "    transforms.Resize((480,640)), \n",
    "    # Convert to grayscale\n",
    "    transforms.Grayscale(num_output_channels=1), \n",
    "])\n",
    "\n",
    "class VideoDataset(Dataset):\n",
    "    def __init__(self, video_files, video_folder, transform=transform):\n",
    "        self.video_folder = video_folder\n",
    "        self.video_files = video_files\n",
    "        self.transform = transform\n",
    "        self.data = self._load_data()\n",
    "\n",
    "    \n",
    "    \n",
    "    def _load_data(self):\n",
    "        data = []\n",
    "        count = 5\n",
    "        \n",
    "        for video_file in self.video_files:\n",
    "            video_path = os.path.join(self.video_folder, video_file)\n",
    "            # If the video name is \"nofixxx.mp4\", this means no-fight --> 0\n",
    "            # If \"NV_xx.mp4\" means no-fight\n",
    "            if \"nofi\" in video_file or \"NV\" in video_file:\n",
    "                label = 0\n",
    "            else:\n",
    "                label = 1\n",
    "                \n",
    "            # Check if we extract the labels correctly\n",
    "            # if count >= 0:\n",
    "            #     print(f'video path: {video_path}')\n",
    "            #     print(f'label: {label}')\n",
    "            #     count -= 1\n",
    "                                      \n",
    "            capture = cv2.VideoCapture(video_path)\n",
    "            fps = int(capture.get(cv2.CAP_PROP_FPS))\n",
    "            total_frames = int(capture.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "             \n",
    "            for idx in range(0, total_frames, fps): # Sample one frame per second\n",
    "                capture.set(cv2.CAP_PROP_POS_FRAMES, idx)\n",
    "                ret, frame = capture.read()\n",
    "                if ret:\n",
    "                    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                    keypoints, image = infer(frame_rgb)\n",
    "                    pose_image = draw_keypoints(keypoints, image, return_kpts=False)\n",
    "                    \n",
    "                    # Check the outputs given by yolov7 output\n",
    "                    # if count >= 0:\n",
    "                    #     plt.figure(figsize=(30, 7))\n",
    "                    #     plt.axis(\"off\")\n",
    "                    #     plt.imshow(pose_image)\n",
    "                    #     plt.savefig(f\"output{count}.jpg\") # Save output if needed\n",
    "                    \n",
    "                    data.append((pose_image, label))\n",
    "            capture.release()\n",
    "        return data\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        pose_image, label = self.data[idx]\n",
    "        if self.transform:\n",
    "            pose_image = self.transform(pose_image)\n",
    "            \n",
    "            # Check the outputs given by yolov7 output witih the added transformations\n",
    "            # if count >= 0:\n",
    "            #     plt.figure(figsize=(30, 7))\n",
    "            #     plt.axis(\"off\")\n",
    "            #     plt.imshow(pose_image)\n",
    "            #     plt.savefig(f\"output{count}.jpg\") # Save output if needed\n",
    "                    \n",
    "        return pose_image, label\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df74bf1a-58ab-4de4-a2eb-b9a72fcf9be6",
   "metadata": {},
   "source": [
    "### Prepare video paths and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9d1ca897-2108-45c1-891a-77c7abfd28b4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-19T04:36:51.196068Z",
     "iopub.status.busy": "2025-04-19T04:36:51.196068Z",
     "iopub.status.idle": "2025-04-19T04:43:58.904398Z",
     "shell.execute_reply": "2025-04-19T04:43:58.903616Z",
     "shell.execute_reply.started": "2025-04-19T04:36:51.196068Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing dataloaders...\n",
      "Splitting data...\n",
      "Train dataset compiled...\n",
      "Validation dataset compiled...\n",
      "Test dataset compiled...\n",
      "Train dataloader compiled...\n",
      "Validation dataloader compiled...\n",
      "Test dataloader compiled...\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "print(\"Preparing dataloaders...\")\n",
    "\n",
    "video_folder = \"../fight-detection-4\"\n",
    "video_files = [f for f in os.listdir(video_folder) if f.endswith((\".mp4\", \".avi\", \".mov\", \".mpeg\"))]\n",
    "\n",
    "###############################################################################\n",
    "\n",
    "print(\"Splitting data...\")\n",
    "# Split dataset such that 20% of data is used for testing, remaining 80% for training + validation\n",
    "train_files, test_files = train_test_split(video_files, test_size=0.2, random_state=42)\n",
    "# Split dataset such that 60% of data is used for training, remaining 20% for validation\n",
    "train_files, val_files = train_test_split(train_files, test_size=0.25, random_state=42) # 0.25 * 0.8 = 0.2\n",
    "\n",
    "###############################################################################\n",
    "\n",
    "train_dataset = VideoDataset(train_files, video_folder)\n",
    "print(\"Train dataset compiled...\")\n",
    "val_dataset = VideoDataset(val_files, video_folder)\n",
    "print(\"Validation dataset compiled...\")\n",
    "test_dataset = VideoDataset(test_files, video_folder)\n",
    "print(\"Test dataset compiled...\")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "print(\"Train dataloader compiled...\")\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "print(\"Validation dataloader compiled...\")\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "print(\"Test dataloader compiled...\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d5ef953-06e4-4480-b533-9bc22b0bd957",
   "metadata": {},
   "source": [
    "## Create instance of Feature extractor (Xception) + LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8020beae-40c9-4c34-b5da-c09e80081b8f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-19T08:51:16.480607Z",
     "iopub.status.busy": "2025-04-19T08:51:16.480115Z",
     "iopub.status.idle": "2025-04-19T08:51:16.616403Z",
     "shell.execute_reply": "2025-04-19T08:51:16.615179Z",
     "shell.execute_reply.started": "2025-04-19T08:51:16.480562Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# torch.cuda.empty_cache()\n",
    "\n",
    "!export PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:128\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "74bf612c-4a0e-4ab8-9491-b210f348682d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-19T08:51:33.924477Z",
     "iopub.status.busy": "2025-04-19T08:51:33.924116Z",
     "iopub.status.idle": "2025-04-19T08:51:34.180269Z",
     "shell.execute_reply": "2025-04-19T08:51:34.178838Z",
     "shell.execute_reply.started": "2025-04-19T08:51:33.924451Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Apr 19 08:51:33 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.144.03             Driver Version: 550.144.03     CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  Quadro P6000                   Off |   00000000:00:05.0 Off |                  Off |\n",
      "| 26%   33C    P0             59W /  250W |    1699MiB /  24576MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "+-----------------------------------------------------------------------------------------+\n",
      "WARNING: infoROM is corrupted at gpu 0000:00:05.0\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9490f920-1c5b-4d65-b219-15fe300fd362",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-19T08:51:53.623810Z",
     "iopub.status.busy": "2025-04-19T08:51:53.623412Z",
     "iopub.status.idle": "2025-04-19T08:51:54.812872Z",
     "shell.execute_reply": "2025-04-19T08:51:54.812872Z",
     "shell.execute_reply.started": "2025-04-19T08:51:53.623785Z"
    }
   },
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 466.00 MiB. GPU 0 has a total capacty of 23.86 GiB of which 334.12 MiB is free. Process 682018 has 23.53 GiB memory in use. Of the allocated memory 23.02 GiB is allocated by PyTorch, and 347.40 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 35\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# Example input for checking if models are loaded correctly\u001b[39;00m\n\u001b[1;32m     34\u001b[0m input_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m16\u001b[39m, \u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m224\u001b[39m, \u001b[38;5;241m224\u001b[39m) \u001b[38;5;66;03m# (batch_size, seq_length, channels, height, width)\u001b[39;00m\n\u001b[0;32m---> 35\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_cnn_lstm\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_tensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPredicted result: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[10], line 15\u001b[0m, in \u001b[0;36mCNN_LSTM.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     13\u001b[0m batch_size, seq_length, c, h, w \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39msize()\n\u001b[1;32m     14\u001b[0m c_in \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mview(batch_size \u001b[38;5;241m*\u001b[39m seq_length, c, h, w)\n\u001b[0;32m---> 15\u001b[0m c_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcnn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mc_in\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m c_out \u001b[38;5;241m=\u001b[39m c_out\u001b[38;5;241m.\u001b[39mview(batch_size, seq_length, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     17\u001b[0m lstm_out, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlstm(c_out)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/timm/models/xception.py:219\u001b[0m, in \u001b[0;36mXception.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m--> 219\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    220\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward_head(x)\n\u001b[1;32m    221\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/timm/models/xception.py:186\u001b[0m, in \u001b[0;36mXception.forward_features\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    183\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn1(x)\n\u001b[1;32m    184\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact1(x)\n\u001b[0;32m--> 186\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    187\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn2(x)\n\u001b[1;32m    188\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact2(x)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py:460\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    459\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 460\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py:456\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    453\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    454\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    455\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 456\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    457\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 466.00 MiB. GPU 0 has a total capacty of 23.86 GiB of which 334.12 MiB is free. Process 682018 has 23.53 GiB memory in use. Of the allocated memory 23.02 GiB is allocated by PyTorch, and 347.40 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import timm\n",
    "\n",
    "class CNN_LSTM(nn.Module):\n",
    "    def __init__(self, cnn_model, hidden_size, num_layers):\n",
    "        super(CNN_LSTM, self).__init__()\n",
    "        self.cnn = cnn_model\n",
    "        self.lstm = nn.LSTM(input_size=cnn_model.num_features, hidden_size=hidden_size, num_layers=num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        batch_size, seq_length, c, h, w = x.size()\n",
    "        c_in = x.view(batch_size * seq_length, c, h, w)\n",
    "        c_out = self.cnn(c_in)\n",
    "        c_out = c_out.view(batch_size, seq_length, -1)\n",
    "        lstm_out, _ = self.lstm(c_out)\n",
    "        out = self.fc(lstm_out[:, -1, :])\n",
    "        return torch.sigmoid(out)\n",
    "\n",
    "# Example usage\n",
    "cnn_model = timm.create_model('xception', pretrained=True)\n",
    "cnn_model.fc = nn.Identity() # Remove the final classification layer\n",
    "model_cnn_lstm = CNN_LSTM(cnn_model, hidden_size=128, num_layers=2)\n",
    "\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_cnn_lstm.to(device)\n",
    "\n",
    "\n",
    "# Example input for checking if models are loaded correctly\n",
    "input_tensor = torch.randn(16, 10, 3, 224, 224) # (batch_size, seq_length, channels, height, width)\n",
    "output = model_cnn_lstm(input_tensor.to(device))\n",
    "print(f\"Predicted result: {output}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eb40650",
   "metadata": {},
   "source": [
    "## Train Xception + LSTM on fighting and non-fighting datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd9fd588-49e3-4498-9588-8451a3afe2db",
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-04-19T04:43:59.698951Z",
     "iopub.status.busy": "2025-04-19T04:43:59.698951Z",
     "iopub.status.idle": "2025-04-19T06:08:52.841171Z",
     "shell.execute_reply": "2025-04-19T06:08:52.841171Z",
     "shell.execute_reply.started": "2025-04-19T04:43:59.698951Z"
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def validate(model, val_loader):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    val_loss = 0.0\n",
    "    correct, total = 0, 0\n",
    "    criterion = nn.BCEWithLogitsLoss()  # Ensure the same loss function is used\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient computation\n",
    "        for pose_img, labels in val_loader:\n",
    "            pose_img = pose_img.repeat(1, 3, 1, 1) # Duplicate the grayscale channel to create a 3-channel image\n",
    "            pose_img, labels = pose_img.to(device), labels.to(device).float()\n",
    "            outputs = model(pose_img)\n",
    "            loss = criterion(outputs, labels.unsqueeze(1))\n",
    "            val_loss += loss.item() * pose_img.size(0)  # Accumulate the loss\n",
    "            \n",
    "            # Calculate accuracy\n",
    "            predicted = torch.round(torch.sigmoid(outputs))\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted.cpu() == labels.cpu().unsqueeze(1)).sum().item()\n",
    "\n",
    "    val_loss /= len(val_loader.dataset)  # Compute the average loss\n",
    "    val_accuracy = correct / total\n",
    "    return val_loss, val_accuracy\n",
    "\n",
    "num_epochs = 100\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "train_accuracies = []\n",
    "val_accuracies = []\n",
    "\n",
    "\n",
    "# Define loss function and optimiser\n",
    "criterion = nn.BCEWithLogitsLoss()  # Binary Cross-Entropy Loss\n",
    "optimizer = torch.optim.Adam(model_resnet.fc.parameters(), lr=0.0001)\n",
    "\n",
    "# count = 1\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model_cnn_lstm.train()\n",
    "    \n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for pose_img, labels in train_loader:\n",
    "        pose_img = pose_img.repeat(1, 3, 1, 1) # Duplicate the grayscale channel to create a 3-channel image\n",
    "        inputs, labels = pose_img.to(device), labels.to(device).float()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model_cnn_lstm(inputs)\n",
    "        loss = criterion(outputs, labels.unsqueeze(1).float())\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        predicted = torch.round(torch.sigmoid(outputs))\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted.cpu() == labels.cpu().unsqueeze(1)).sum().item()\n",
    "        \n",
    "        # if count >= 0:\n",
    "        #     print(f\"Predicted value: {predicted.cpu()}\")\n",
    "        #     print(f\"Label: {labels.cpu().unsqueeze(1)}\")\n",
    "        #     count -= 1\n",
    "\n",
    "    epoch_loss = running_loss / len(train_loader.dataset)\n",
    "    epoch_accuracy = correct / total\n",
    "    train_losses.append(epoch_loss)\n",
    "    train_accuracies.append(epoch_accuracy)\n",
    "\n",
    "    # Validation step (assuming val_loader is defined)\n",
    "    val_loss, val_accuracy = validate(model_cnn_lstm, val_loader)\n",
    "    val_losses.append(val_loss)\n",
    "    val_accuracies.append(val_accuracy)\n",
    "    \n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "    \n",
    "    # Save model weights for every 5 epochs\n",
    "    # if epoch % 5 == 0:\n",
    "    #     torch.save(model_cnn_lstm.state_dict(), f'../model_epoch_2_{epoch+1}.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "272945e3-cb04-485e-82f8-24c8bda283da",
   "metadata": {},
   "source": [
    "## Save model weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ca2eb716-2a4b-4035-b250-ba1748410436",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-19T06:08:52.843328Z",
     "iopub.status.busy": "2025-04-19T06:08:52.843328Z",
     "iopub.status.idle": "2025-04-19T06:08:53.018618Z",
     "shell.execute_reply": "2025-04-19T06:08:53.017754Z",
     "shell.execute_reply.started": "2025-04-19T06:08:52.843328Z"
    }
   },
   "outputs": [],
   "source": [
    "torch.save(model_cnn_lstm.state_dict(), f'../model_epoch_recent.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf95846-36f2-4b65-b538-f3604910da6e",
   "metadata": {},
   "source": [
    "## Show training-val loss and accuracy over epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11da6e3a-99bf-4c27-9c20-822523b09c7f",
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-04-19T06:08:53.018618Z",
     "iopub.status.busy": "2025-04-19T06:08:53.018618Z",
     "iopub.status.idle": "2025-04-19T06:08:53.580413Z",
     "shell.execute_reply": "2025-04-19T06:08:53.575533Z",
     "shell.execute_reply.started": "2025-04-19T06:08:53.018618Z"
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# Plot training and validation loss\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(range(1, num_epochs+1), train_losses, label='Training Loss')\n",
    "plt.plot(range(1, num_epochs+1), val_losses, label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.show()\n",
    "\n",
    "# Plot training and validation accuracy\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(range(1, num_epochs+1), train_accuracies, label='Training Accuracy')\n",
    "plt.plot(range(1, num_epochs+1), val_accuracies, label='Validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6247b596-6e48-4555-a732-4b0ca2180bb9",
   "metadata": {},
   "source": [
    "## Basic inference and testing - for one video file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a136c1d-028f-4f65-aa54-8128e30f6e26",
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-04-19T04:35:46.688447Z",
     "iopub.status.busy": "2025-04-19T04:35:46.688447Z",
     "iopub.status.idle": "2025-04-19T04:35:48.045057Z",
     "shell.execute_reply": "2025-04-19T04:35:48.044296Z",
     "shell.execute_reply.started": "2025-04-19T04:35:46.688447Z"
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Load weights\n",
    "model_cnn_lstm.load_state_dict(torch.load('../model_epoch_recent.pth'))\n",
    "model_cnn_lstm.eval()\n",
    "\n",
    "# Pre-process input --> extract frames\n",
    "video_path = '../fight-detection-2/nofi096.mp4'\n",
    "\n",
    "class Video(Dataset):\n",
    "    def __init__(self, video_path, transform=transform):\n",
    "        self.video_path = video_path\n",
    "        self.transform = transform\n",
    "        self.data = self._load_data()\n",
    "\n",
    "    def _load_data(self):\n",
    "        data = []\n",
    "                                      \n",
    "        capture = cv2.VideoCapture(self.video_path)\n",
    "        fps = int(capture.get(cv2.CAP_PROP_FPS))\n",
    "        total_frames = int(capture.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "        # for idx in range(0, total_frames, fps): # use this to sample one frame per second\n",
    "        for idx in range(0, total_frames, fps): # use this to sample all frames in the video\n",
    "            capture.set(cv2.CAP_PROP_POS_FRAMES, idx)\n",
    "            ret, frame = capture.read()\n",
    "            if ret:\n",
    "                frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                keypoints, image = infer(frame_rgb)\n",
    "                pose_image = draw_keypoints(keypoints, image, return_kpts=False)\n",
    "                data.append(pose_image)\n",
    "        capture.release()\n",
    "        return data\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        pose_image = self.data[idx]\n",
    "        if self.transform:\n",
    "            pose_image = self.transform(pose_image)\n",
    "        return pose_image\n",
    "\n",
    "processed_video = Video(video_path)\n",
    "print(\"Video compiled...\")\n",
    "\n",
    "video_loader = DataLoader(processed_video, batch_size=32, shuffle=True)     \n",
    "print(\"Video loaded...\")\n",
    "        \n",
    "# inference\n",
    "for pose_img in video_loader:\n",
    "    pose_img = pose_img.repeat(1, 3, 1, 1) # Duplicate the grayscale channel to create a 3-channel image\n",
    "    pose_img = pose_img.to(device)\n",
    "    outputs = model_cnn_lstm(pose_img)\n",
    "    predicted = torch.round(torch.sigmoid(outputs))\n",
    "    print(f\"Predicted result for batch: {predicted}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61a2e3e5-fa54-4f05-a1a2-ff306535ba6f",
   "metadata": {},
   "source": [
    "## Testloader\n",
    "- can be used for unseen dataset\n",
    "- or for the 20% split dataset done earlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "471f69ef-fe07-444c-b12c-d487aab7c782",
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-04-19T06:08:53.580413Z",
     "iopub.status.busy": "2025-04-19T06:08:53.580413Z",
     "iopub.status.idle": "2025-04-19T06:09:07.879332Z",
     "shell.execute_reply": "2025-04-19T06:09:07.879332Z",
     "shell.execute_reply.started": "2025-04-19T06:08:53.580413Z"
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Load weights\n",
    "model_cnn_lstm.load_state_dict(torch.load('../model_epoch_recent.pth'))\n",
    "model_cnn_lstm.eval()\n",
    "\n",
    "def test(model, test_loader=test_loader):\n",
    "    # model.eval()  # Set the model to evaluation mode\n",
    "    test_loss = 0.0\n",
    "    correct, total = 0, 0\n",
    "    criterion = nn.BCEWithLogitsLoss()  # Ensure the same loss function is used\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient computation\n",
    "        for pose_img, labels in test_loader:\n",
    "            pose_img = pose_img.repeat(1, 3, 1, 1) # Duplicate the grayscale channel to create a 3-channel image\n",
    "            pose_img, labels = pose_img.to(device), labels.to(device).float()\n",
    "            outputs = model(pose_img)\n",
    "            loss = criterion(outputs, labels.unsqueeze(1))\n",
    "            test_loss += loss.item() * pose_img.size(0)  # Accumulate the loss\n",
    "            \n",
    "            # Calculate accuracy\n",
    "            predicted = torch.round(torch.sigmoid(outputs))\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted.cpu() == labels.cpu().unsqueeze(1)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)  # Compute the average loss\n",
    "    test_accuracy = correct / total\n",
    "    return test_loss, test_accuracy\n",
    "\n",
    "test_loss, test_accuracy = test(model_cnn_lstm)\n",
    "print(f\"Test loss: {test_loss} Test accuracy: {test_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "253744ba-55bc-4854-90eb-3b7ab05a6ec7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
