{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensure `nvidia-smi` works.\n",
    "\n",
    "Try running `!nvidia-smi` to check, else download [CUDA toolkit](https://developer.nvidia.com/cuda-downloads?target_os=Windows&target_arch=x86_64&target_version=11&target_type=exe_local)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "# import urllib.request\n",
    "import sys\n",
    "import torch\n",
    "import time\n",
    "# import datetime\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "# from moviepy.editor import *\n",
    "\n",
    "# %matplotlib inlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/limsophie/Documents/GitHub/Fighting-detection-in-CCTVs/yolov7\n"
     ]
    }
   ],
   "source": [
    "# Change directory to \"/yolov7\"\n",
    "os.chdir(\"yolov7\")\n",
    "print(os.getcwd()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model:  yolov7-tiny...\n",
      "Model loaded.\n"
     ]
    }
   ],
   "source": [
    "from utils.datasets import letterbox\n",
    "from utils.general import non_max_suppression_kpt\n",
    "from utils.plots import output_to_keypoint, plot_skeleton_kpts\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load models\n",
    "print(\"Loading model: \", \"yolov7-tiny...\")\n",
    "weights_path = \"yolov7-w6-pose.pt\"\n",
    "model = torch.load(weights_path, map_location=device, weights_only=False)['model']\n",
    "print(\"Model loaded.\")\n",
    "\n",
    "model.float().eval()\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    # half() turns predictions into float16 tensors --> significantly lowers inference time\n",
    "    model.half().to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.datasets import letterbox\n",
    "\n",
    "# Main inference\n",
    "def infer(image):\n",
    "    image = letterbox(image, 960, \n",
    "                      stride=64,\n",
    "                      auto=True)[0]  # shape: (567, 960, 3)\n",
    "    \n",
    "    image = transforms.ToTensor()(image)  # torch.Size([3, 567, 960])\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        image = image.half().to(device)\n",
    "\n",
    "    image = image.unsqueeze(0)  # torch.Size([1, 3, 567, 960])\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output, _ = model(image)\n",
    "\n",
    "    return output, image\n",
    "\n",
    "\n",
    "def draw_keypoints(output, image, confidence=0.25, threshold=0.65, return_kpts=False):\n",
    "    \"\"\"\n",
    "    Draw YOLOv7 pose keypoints and optionally return keypoints for saving.\n",
    "    \"\"\"\n",
    "    output = non_max_suppression_kpt(\n",
    "        output,\n",
    "        confidence,\n",
    "        threshold,\n",
    "        nc=model.yaml['nc'],\n",
    "        nkpt=model.yaml['nkpt'],\n",
    "        kpt_label=True)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = output_to_keypoint(output)  # shape: (num_people, 51)\n",
    "\n",
    "    # Convert tensor image back to numpy\n",
    "    nimg = image[0].permute(1, 2, 0) * 255\n",
    "    nimg = cv2.cvtColor(nimg.cpu().numpy().astype(np.uint8), cv2.COLOR_RGB2BGR)\n",
    "\n",
    "    # Correctly loop through 'output' variable instead of 'kpts'\n",
    "    for idx in range(output.shape[0]):\n",
    "        plot_skeleton_kpts(nimg, output[idx, 7:].T, 3)\n",
    "\n",
    "    if return_kpts:\n",
    "        return nimg, output  # (image with keypoints drawn, raw keypoints)\n",
    "\n",
    "    return nimg\n",
    "\n",
    "#code had problem = \"NameError: name 'kpts' is not defined\"\n",
    "#above code is the new one but essentially the part that changed was for idx in range...\n",
    "#old code below\n",
    "# for idx in range(kpts.shape[0]):\n",
    "#        plot_skeleton_kpts(nimg, kpts[idx, 7:].T, 3)\n",
    "#\n",
    "#   return nimg, kpts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/limsophie/Documents/GitHub/Fighting-detection-in-CCTVs\n"
     ]
    }
   ],
   "source": [
    "#exit yolov7\n",
    "\n",
    "import os\n",
    "\n",
    "# Step 1: Move up one directory level from the current working directory\n",
    "os.chdir(os.path.dirname(os.getcwd()))  # Exit the current directory = yolov7\n",
    "\n",
    "#os.chdir(\"fighting-detection-in-cctvs\")  # Change working directory\n",
    "\n",
    "print(os.getcwd())  # Prints the current working directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract keypoints from the produced frames already processed by yolov7\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Folder containing your extracted frames (.jpg)\n",
    "frame_folder = \"results/fight_frames\"\n",
    "\n",
    "# Folder to store extracted keypoints (.npy)\n",
    "keypoint_folder = \"results/fight_keypoints\"\n",
    "os.makedirs(keypoint_folder, exist_ok=True)\n",
    "\n",
    "def extract_keypoints_from_frames(frame_folder, keypoint_folder):\n",
    "    frame_files = sorted([f for f in os.listdir(frame_folder) if f.endswith(\".jpg\")])\n",
    "\n",
    "    for frame_file in tqdm(frame_files):\n",
    "        basename = os.path.splitext(frame_file)[0]\n",
    "        npy_path = os.path.join(keypoint_folder, basename + \".npy\")\n",
    "\n",
    "        # Skip if the .npy already exists\n",
    "        if os.path.exists(npy_path):\n",
    "            continue\n",
    "\n",
    "        frame_path = os.path.join(frame_folder, frame_file)\n",
    "        image_bgr = cv2.imread(frame_path)\n",
    "        if image_bgr is None:\n",
    "            print(f\"Could not read {frame_file}\")\n",
    "            continue\n",
    "\n",
    "        # Convert to RGB\n",
    "        image_rgb = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        try:\n",
    "            output, image_tensor = infer(image_rgb)\n",
    "            _, keypoints = draw_keypoints(output, image_tensor, return_kpts=True)\n",
    "\n",
    "            if keypoints is not None and keypoints.shape[0] > 0:\n",
    "                first_person_kpts = keypoints[0, 7:]  # Only the 17 keypoints\n",
    "                keypoints_array = first_person_kpts.reshape(-1, 3)  # Shape: (17, 3)\n",
    "\n",
    "                # Save as .npy\n",
    "                np.save(npy_path, keypoints_array)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {frame_file}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (don't run this) draws points on the boxing photo\n",
    "import matplotlib\n",
    "matplotlib.use('MacOSX')\n",
    "\n",
    "imagefile = \"../test_images/boxing_grey.jpg\"\n",
    "\n",
    "print(\"Inferencing image input...\")\n",
    "\n",
    "output, image = infer(cv2.imread(imagefile))\n",
    "pose_image = draw_keypoints(output, image, confidence=0.25, threshold=0.65)\n",
    "\n",
    "print(\"Inference complete.\")\n",
    "\n",
    "plt.figure(figsize=(30, 7))\n",
    "plt.axis(\"off\")\n",
    "plt.imshow(pose_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#old code don't use\n",
    "import os\n",
    "import time\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def yoloV7_pose_video(videofile, confidence=0.25, threshold=0.65):\n",
    "    start = time.time()\n",
    "\n",
    "    capture = cv2.VideoCapture(videofile)\n",
    "    fps = capture.get(cv2.CAP_PROP_FPS)\n",
    "    total_frames = capture.get(cv2.CAP_PROP_FRAME_COUNT)\n",
    "\n",
    "    print(f\"Processing video: {videofile}\")\n",
    "    \n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    outputvideofile = \"../results/result_\" + os.path.basename(videofile)\n",
    "    outvideo = cv2.VideoWriter(outputvideofile, fourcc, 30.0,\n",
    "                               (int(capture.get(3)), int(capture.get(4))))\n",
    "\n",
    "    # Make sure output directories exist\n",
    "    os.makedirs(\"results/fight_frames\", exist_ok=True)\n",
    "    os.makedirs(\"results/fight_keypoints\", exist_ok=True)\n",
    "\n",
    "    idx = 1\n",
    "    while capture.isOpened():\n",
    "        ret, frame = capture.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        if idx % fps == 1:\n",
    "            print(\"Processed frames =\", f\"{idx:06}\")\n",
    "\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        output, tensor_frame = infer(frame_rgb)\n",
    "        \n",
    "        frame_with_kpts, keypoints = draw_keypoints(output, tensor_frame, confidence, threshold, return_kpts=True)\n",
    "\n",
    "        # Save keypoints (only if keypoints are found)\n",
    "        if keypoints is not None and keypoints.shape[0] > 0:\n",
    "            first_person_kpts = keypoints[0, 7:]  # shape: (51,)\n",
    "            keypoints_array = first_person_kpts.reshape(-1, 3)  # (17, 3)\n",
    "\n",
    "            npy_filename = os.path.join(\"results/fight_keypoints\", f\"frame_{os.path.basename(videofile)}_{idx:06}.npy\")\n",
    "            np.save(npy_filename, keypoints_array)\n",
    "\n",
    "        # Save visual frame\n",
    "        frame_with_kpts = cv2.resize(frame_with_kpts,\n",
    "                                     (int(capture.get(3)), int(capture.get(4))))\n",
    "        jpg_filename = os.path.join(\"results/fight_frames\", f\"videoframe_{os.path.basename(videofile)}_{idx:06}.jpg\")\n",
    "        cv2.imwrite(jpg_filename, frame_with_kpts)\n",
    "\n",
    "        outvideo.write(frame_with_kpts)\n",
    "        idx += 1\n",
    "\n",
    "    capture.release()\n",
    "    outvideo.release()\n",
    "    print(f\"Done. Output video: {outputvideofile}\")\n",
    "    return outputvideofile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use this one\n",
    "\n",
    "import os\n",
    "import time\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def yoloV7_pose_video(videofile, frame_dir, keypoint_dir, confidence=0.25, threshold=0.65):\n",
    "    start = time.time()\n",
    "\n",
    "    capture = cv2.VideoCapture(videofile)\n",
    "    fps = capture.get(cv2.CAP_PROP_FPS)\n",
    "    total_frames = capture.get(cv2.CAP_PROP_FRAME_COUNT)\n",
    "\n",
    "    print(f\"Processing video: {videofile}\")\n",
    "    \n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    outputvideofile = \"../results/result_\" + os.path.basename(videofile)\n",
    "    outvideo = cv2.VideoWriter(outputvideofile, fourcc, 30.0,\n",
    "                               (int(capture.get(3)), int(capture.get(4))))\n",
    "\n",
    "    # Create the output folders if they don't exist\n",
    "    os.makedirs(frame_dir, exist_ok=True)\n",
    "    os.makedirs(keypoint_dir, exist_ok=True)\n",
    "\n",
    "    idx = 1\n",
    "    while capture.isOpened():\n",
    "        ret, frame = capture.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        if idx % fps == 1:\n",
    "            print(\"Processed frames =\", f\"{idx:06}\")\n",
    "\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        output, tensor_frame = infer(frame_rgb)\n",
    "        \n",
    "        frame_with_kpts, keypoints = draw_keypoints(output, tensor_frame, confidence, threshold, return_kpts=True)\n",
    "\n",
    "        # Save keypoints\n",
    "        if keypoints is not None and keypoints.shape[0] > 0:\n",
    "            first_person_kpts = keypoints[0, 7:]  # shape: (51,)\n",
    "            keypoints_array = first_person_kpts.reshape(-1, 3)  # (17, 3)\n",
    "\n",
    "            npy_filename = os.path.join(keypoint_dir, f\"frame_{os.path.basename(videofile)}_{idx:06}.npy\")\n",
    "            np.save(npy_filename, keypoints_array)\n",
    "\n",
    "        # Save visual frame\n",
    "        frame_with_kpts = cv2.resize(frame_with_kpts,\n",
    "                                     (int(capture.get(3)), int(capture.get(4))))\n",
    "        jpg_filename = os.path.join(frame_dir, f\"videoframe_{os.path.basename(videofile)}_{idx:06}.jpg\")\n",
    "        cv2.imwrite(jpg_filename, frame_with_kpts)\n",
    "\n",
    "        outvideo.write(frame_with_kpts)\n",
    "        idx += 1\n",
    "\n",
    "    capture.release()\n",
    "    outvideo.release()\n",
    "    print(f\"Done. Output video: {outputvideofile}\")\n",
    "    return outputvideofile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting pose extraction on train_videos/nf_train_40/nofi038.mp4...\n",
      "Processing video: train_videos/nf_train_40/nofi038.mp4\n",
      "Processed frames = 000001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/limsophie/anaconda3/lib/python3.11/site-packages/torch/functional.py:507: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/TensorShape.cpp:3550.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed frames = 000031\n",
      "Done. Output video: ../results/result_nofi038.mp4\n",
      "Starting pose extraction on train_videos/nf_train_40/nofi010.mp4...\n",
      "Processing video: train_videos/nf_train_40/nofi010.mp4\n",
      "Processed frames = 000001\n",
      "Done. Output video: ../results/result_nofi010.mp4\n",
      "Starting pose extraction on train_videos/nf_train_40/nofi004.mp4...\n",
      "Processing video: train_videos/nf_train_40/nofi004.mp4\n",
      "Processed frames = 000001\n",
      "Processed frames = 000026\n",
      "Done. Output video: ../results/result_nofi004.mp4\n",
      "Starting pose extraction on train_videos/nf_train_40/nofi005.mp4...\n",
      "Processing video: train_videos/nf_train_40/nofi005.mp4\n",
      "Processed frames = 000001\n",
      "Processed frames = 000026\n",
      "Done. Output video: ../results/result_nofi005.mp4\n",
      "Starting pose extraction on train_videos/nf_train_40/nofi011.mp4...\n",
      "Processing video: train_videos/nf_train_40/nofi011.mp4\n",
      "Processed frames = 000001\n",
      "Done. Output video: ../results/result_nofi011.mp4\n",
      "Starting pose extraction on train_videos/nf_train_40/nofi039.mp4...\n",
      "Processing video: train_videos/nf_train_40/nofi039.mp4\n",
      "Processed frames = 000001\n",
      "Processed frames = 000031\n",
      "Processed frames = 000061\n",
      "Done. Output video: ../results/result_nofi039.mp4\n",
      "Starting pose extraction on train_videos/nf_train_40/nofi007.mp4...\n",
      "Processing video: train_videos/nf_train_40/nofi007.mp4\n",
      "Processed frames = 000001\n",
      "Processed frames = 000013\n",
      "Processed frames = 000025\n",
      "Done. Output video: ../results/result_nofi007.mp4\n",
      "Starting pose extraction on train_videos/nf_train_40/nofi013.mp4...\n",
      "Processing video: train_videos/nf_train_40/nofi013.mp4\n",
      "Processed frames = 000001\n",
      "Processed frames = 000026\n",
      "Done. Output video: ../results/result_nofi013.mp4\n",
      "Starting pose extraction on train_videos/nf_train_40/nofi012.mp4...\n",
      "Processing video: train_videos/nf_train_40/nofi012.mp4\n",
      "Processed frames = 000001\n",
      "Processed frames = 000026\n",
      "Done. Output video: ../results/result_nofi012.mp4\n",
      "Starting pose extraction on train_videos/nf_train_40/nofi006.mp4...\n",
      "Processing video: train_videos/nf_train_40/nofi006.mp4\n",
      "Processed frames = 000001\n",
      "Done. Output video: ../results/result_nofi006.mp4\n",
      "Starting pose extraction on train_videos/nf_train_40/nofi002.mp4...\n",
      "Processing video: train_videos/nf_train_40/nofi002.mp4\n",
      "Processed frames = 000001\n",
      "Processed frames = 000026\n",
      "Processed frames = 000051\n",
      "Processed frames = 000076\n",
      "Done. Output video: ../results/result_nofi002.mp4\n",
      "Starting pose extraction on train_videos/nf_train_40/nofi016.mp4...\n",
      "Processing video: train_videos/nf_train_40/nofi016.mp4\n",
      "Processed frames = 000001\n",
      "Processed frames = 000031\n",
      "Processed frames = 000061\n",
      "Done. Output video: ../results/result_nofi016.mp4\n",
      "Starting pose extraction on train_videos/nf_train_40/nofi017.mp4...\n",
      "Processing video: train_videos/nf_train_40/nofi017.mp4\n",
      "Processed frames = 000001\n",
      "Processed frames = 000031\n",
      "Processed frames = 000061\n",
      "Done. Output video: ../results/result_nofi017.mp4\n",
      "Starting pose extraction on train_videos/nf_train_40/nofi003.mp4...\n",
      "Processing video: train_videos/nf_train_40/nofi003.mp4\n",
      "Processed frames = 000001\n",
      "Processed frames = 000026\n",
      "Processed frames = 000051\n",
      "Done. Output video: ../results/result_nofi003.mp4\n",
      "Starting pose extraction on train_videos/nf_train_40/nofi015.mp4...\n",
      "Processing video: train_videos/nf_train_40/nofi015.mp4\n",
      "Processed frames = 000001\n",
      "Done. Output video: ../results/result_nofi015.mp4\n",
      "Starting pose extraction on train_videos/nf_train_40/nofi001.mp4...\n",
      "Processing video: train_videos/nf_train_40/nofi001.mp4\n",
      "Processed frames = 000001\n",
      "Processed frames = 000026\n",
      "Processed frames = 000051\n",
      "Processed frames = 000076\n",
      "Done. Output video: ../results/result_nofi001.mp4\n",
      "Starting pose extraction on train_videos/nf_train_40/nofi029.mp4...\n",
      "Processing video: train_videos/nf_train_40/nofi029.mp4\n",
      "Processed frames = 000001\n",
      "Done. Output video: ../results/result_nofi029.mp4\n",
      "Starting pose extraction on train_videos/nf_train_40/nofi028.mp4...\n",
      "Processing video: train_videos/nf_train_40/nofi028.mp4\n",
      "Processed frames = 000001\n",
      "Done. Output video: ../results/result_nofi028.mp4\n",
      "Starting pose extraction on train_videos/nf_train_40/nofi014.mp4...\n",
      "Processing video: train_videos/nf_train_40/nofi014.mp4\n",
      "Processed frames = 000001\n",
      "Done. Output video: ../results/result_nofi014.mp4\n",
      "Starting pose extraction on train_videos/nf_train_40/nofi040.mp4...\n",
      "Processing video: train_videos/nf_train_40/nofi040.mp4\n",
      "Processed frames = 000001\n",
      "Processed frames = 000031\n",
      "Done. Output video: ../results/result_nofi040.mp4\n",
      "Starting pose extraction on train_videos/nf_train_40/nofi019.mp4...\n",
      "Processing video: train_videos/nf_train_40/nofi019.mp4\n",
      "Processed frames = 000001\n",
      "Done. Output video: ../results/result_nofi019.mp4\n",
      "Starting pose extraction on train_videos/nf_train_40/nofi031.mp4...\n",
      "Processing video: train_videos/nf_train_40/nofi031.mp4\n",
      "Processed frames = 000001\n",
      "Done. Output video: ../results/result_nofi031.mp4\n",
      "Starting pose extraction on train_videos/nf_train_40/nofi025.mp4...\n",
      "Processing video: train_videos/nf_train_40/nofi025.mp4\n",
      "Processed frames = 000001\n",
      "Done. Output video: ../results/result_nofi025.mp4\n",
      "Starting pose extraction on train_videos/nf_train_40/nofi024.mp4...\n",
      "Processing video: train_videos/nf_train_40/nofi024.mp4\n",
      "Processed frames = 000001\n",
      "Done. Output video: ../results/result_nofi024.mp4\n",
      "Starting pose extraction on train_videos/nf_train_40/nofi030.mp4...\n",
      "Processing video: train_videos/nf_train_40/nofi030.mp4\n",
      "Processed frames = 000001\n",
      "Done. Output video: ../results/result_nofi030.mp4\n",
      "Starting pose extraction on train_videos/nf_train_40/nofi018.mp4...\n",
      "Processing video: train_videos/nf_train_40/nofi018.mp4\n",
      "Processed frames = 000001\n",
      "Done. Output video: ../results/result_nofi018.mp4\n",
      "Starting pose extraction on train_videos/nf_train_40/nofi026.mp4...\n",
      "Processing video: train_videos/nf_train_40/nofi026.mp4\n",
      "Processed frames = 000001\n",
      "Processed frames = 000031\n",
      "Done. Output video: ../results/result_nofi026.mp4\n",
      "Starting pose extraction on train_videos/nf_train_40/nofi032.mp4...\n",
      "Processing video: train_videos/nf_train_40/nofi032.mp4\n",
      "Processed frames = 000001\n",
      "Done. Output video: ../results/result_nofi032.mp4\n",
      "Starting pose extraction on train_videos/nf_train_40/nofi033.mp4...\n",
      "Processing video: train_videos/nf_train_40/nofi033.mp4\n",
      "Processed frames = 000001\n",
      "Done. Output video: ../results/result_nofi033.mp4\n",
      "Starting pose extraction on train_videos/nf_train_40/nofi027.mp4...\n",
      "Processing video: train_videos/nf_train_40/nofi027.mp4\n",
      "Processed frames = 000001\n",
      "Processed frames = 000031\n",
      "Done. Output video: ../results/result_nofi027.mp4\n",
      "Starting pose extraction on train_videos/nf_train_40/nofi023.mp4...\n",
      "Processing video: train_videos/nf_train_40/nofi023.mp4\n",
      "Processed frames = 000001\n",
      "Done. Output video: ../results/result_nofi023.mp4\n",
      "Starting pose extraction on train_videos/nf_train_40/nofi037.mp4...\n",
      "Processing video: train_videos/nf_train_40/nofi037.mp4\n",
      "Processed frames = 000001\n",
      "Processed frames = 000031\n",
      "Processed frames = 000061\n",
      "Done. Output video: ../results/result_nofi037.mp4\n",
      "Starting pose extraction on train_videos/nf_train_40/nofi036.mp4...\n",
      "Processing video: train_videos/nf_train_40/nofi036.mp4\n",
      "Processed frames = 000001\n",
      "Processed frames = 000026\n",
      "Done. Output video: ../results/result_nofi036.mp4\n",
      "Starting pose extraction on train_videos/nf_train_40/nofi022.mp4...\n",
      "Processing video: train_videos/nf_train_40/nofi022.mp4\n",
      "Processed frames = 000001\n",
      "Done. Output video: ../results/result_nofi022.mp4\n",
      "Starting pose extraction on train_videos/nf_train_40/nofi034.mp4...\n",
      "Processing video: train_videos/nf_train_40/nofi034.mp4\n",
      "Processed frames = 000001\n",
      "Processed frames = 000026\n",
      "Done. Output video: ../results/result_nofi034.mp4\n",
      "Starting pose extraction on train_videos/nf_train_40/nofi020.mp4...\n",
      "Processing video: train_videos/nf_train_40/nofi020.mp4\n",
      "Processed frames = 000001\n",
      "Done. Output video: ../results/result_nofi020.mp4\n",
      "Starting pose extraction on train_videos/nf_train_40/nofi008.mp4...\n",
      "Processing video: train_videos/nf_train_40/nofi008.mp4\n",
      "Processed frames = 000001\n",
      "Processed frames = 000013\n",
      "Processed frames = 000025\n",
      "Done. Output video: ../results/result_nofi008.mp4\n",
      "Starting pose extraction on train_videos/nf_train_40/nofi009.mp4...\n",
      "Processing video: train_videos/nf_train_40/nofi009.mp4\n",
      "Processed frames = 000001\n",
      "Done. Output video: ../results/result_nofi009.mp4\n",
      "Starting pose extraction on train_videos/nf_train_40/nofi021.mp4...\n",
      "Processing video: train_videos/nf_train_40/nofi021.mp4\n",
      "Processed frames = 000001\n",
      "Done. Output video: ../results/result_nofi021.mp4\n",
      "Starting pose extraction on train_videos/nf_train_40/nofi035.mp4...\n",
      "Processing video: train_videos/nf_train_40/nofi035.mp4\n",
      "Processed frames = 000001\n",
      "Processed frames = 000026\n",
      "Done. Output video: ../results/result_nofi035.mp4\n",
      "✅ All non-fight videos processed!\n"
     ]
    }
   ],
   "source": [
    "#run yoloV7_pose_video on a whole folder (be careful)\n",
    "import os\n",
    "\n",
    "# Your video folder\n",
    "video_folder = \"train_videos/nf_train_40\"\n",
    "\n",
    "# Output folders\n",
    "frame_dir = \"results/nonfight_frames\"\n",
    "keypoint_dir = \"results/nonfight_keypoints\"\n",
    "\n",
    "# Make sure output folders exist\n",
    "os.makedirs(frame_dir, exist_ok=True)\n",
    "os.makedirs(keypoint_dir, exist_ok=True)\n",
    "\n",
    "# Get list of video files (mp4, avi, etc.)\n",
    "video_files = [f for f in os.listdir(video_folder) if f.endswith((\".mp4\", \".avi\", \".mov\"))]\n",
    "\n",
    "# Loop through and run pose extraction\n",
    "for video in video_files:\n",
    "    video_path = os.path.join(video_folder, video)\n",
    "    print(f\"Starting pose extraction on {video_path}...\")\n",
    "\n",
    "    yoloV7_pose_video(\n",
    "        videofile=video_path,\n",
    "        frame_dir=frame_dir,\n",
    "        keypoint_dir=keypoint_dir\n",
    "    )\n",
    "\n",
    "print(\"✅ All non-fight videos processed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing video: train_videos/train_40/fight_0015.mpeg\n",
      "Processed frames = 000001\n",
      "Done. Output video: ../results/result_fight_0015.mpeg\n"
     ]
    }
   ],
   "source": [
    "#fight_0086 = \"train_videos/train_40/fight_0086.mpeg\"\n",
    "#video_52 = yoloV7_pose_video(fight_0086)\n",
    "\n",
    "#fight_0075 = \"train_videos/train_40/fight_0075.mpeg\"\n",
    "#video_27 = yoloV7_pose_video(fight_0075)\n",
    "\n",
    "#fight_0064 = \"train_videos/train_40/fight_0064.mpeg\"\n",
    "#video_16 = yoloV7_pose_video(fight_0064)\n",
    "\n",
    "#fight_0021 (13 sec), fight_0022 (12 sec), fight_0062 (edit the clip)\n",
    "\n",
    "#fight_0063 = \"train_videos/train_40/fight_0063.mpeg\"\n",
    "#video_15 = yoloV7_pose_video(fight_0063)\n",
    "\n",
    "#fight_0009 = \"train_videos/train_40/fight_0009.mpeg\"\n",
    "#video_34 = yoloV7_pose_video(fight_0009)\n",
    "\n",
    "#fight_0015 = \"train_videos/train_40/fight_0015.mpeg\"\n",
    "#video_30 = yoloV7_pose_video(fight_0015)\n",
    "\n",
    "#fight_0019 = \"train_videos/train_40/fight_0019.mpeg\"\n",
    "#video_05 = yoloV7_pose_video(fight_0019)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2015/2015 [1:46:18<00:00,  3.17s/it]  \n"
     ]
    }
   ],
   "source": [
    "#old code\n",
    "#extract_keypoints_from_frames(frame_folder, keypoint_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2351 [00:00<?, ?it/s]/Users/limsophie/anaconda3/lib/python3.11/site-packages/torch/functional.py:507: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/TensorShape.cpp:3550.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
      "100%|██████████| 2351/2351 [1:48:51<00:00,  2.78s/it]  \n"
     ]
    }
   ],
   "source": [
    "#hide one to run the other if you want, I usually run them one by one\n",
    "extract_keypoints_from_frames(\"results/nonfight_frames\", \"results/nonfight_keypoints\")\n",
    "extract_keypoints_from_frames(\"results/fight_frames\", \"results/nonfight_keypoints\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#string together the related keyframes into a sequence for learning\n",
    "from torch.utils.data import Dataset\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "class PoseSequenceDataset(Dataset):\n",
    "    def __init__(self, frame_dir, sequence_len=30, label=1):  # <-- This line is key\n",
    "        self.frame_dir = frame_dir\n",
    "        self.sequence_len = sequence_len\n",
    "        self.label = label\n",
    "\n",
    "        # Sort all keypoint files\n",
    "        self.keypoint_files = sorted([\n",
    "            os.path.join(frame_dir, f) for f in os.listdir(frame_dir) if f.endswith('.npy')\n",
    "        ])\n",
    "\n",
    "        # Create sequences\n",
    "        self.sequences = [\n",
    "            self.keypoint_files[i:i+sequence_len]\n",
    "            for i in range(0, len(self.keypoint_files) - sequence_len + 1, sequence_len)\n",
    "        ]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sequence_paths = self.sequences[idx]\n",
    "        sequence = np.array([np.load(p)[:, :2] for p in sequence_paths])  # (seq_len, 17, 2)\n",
    "        sequence = sequence.reshape(self.sequence_len, -1)  # (seq_len, 34)\n",
    "        return torch.tensor(sequence, dtype=torch.float32), torch.tensor(self.label, dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class FightClassifierLSTM(nn.Module):\n",
    "    def __init__(self, input_size=34, hidden_size=64, num_layers=2, num_classes=2):\n",
    "        super(FightClassifierLSTM, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)  # x: (batch, seq_len, input_size)\n",
    "        out = out[:, -1, :]    # use last time step\n",
    "        out = self.fc(out)     # output logits\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "def train_model(model, train_loader, num_epochs=10, lr=1e-3, device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
    "    model = model.to(device)\n",
    "    model.train()  # Set to training mode\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        for sequences, labels in train_loader:\n",
    "            sequences = sequences.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(sequences)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        accuracy = 100 * correct / total\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}] - Loss: {avg_loss:.4f} | Accuracy: {accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10] - Loss: 0.6690 | Accuracy: 55.70%\n",
      "Epoch [2/10] - Loss: 0.5547 | Accuracy: 76.79%\n",
      "Epoch [3/10] - Loss: 0.5849 | Accuracy: 69.20%\n",
      "Epoch [4/10] - Loss: 0.5483 | Accuracy: 72.57%\n",
      "Epoch [5/10] - Loss: 0.4711 | Accuracy: 78.06%\n",
      "Epoch [6/10] - Loss: 0.6046 | Accuracy: 67.51%\n",
      "Epoch [7/10] - Loss: 0.6518 | Accuracy: 62.03%\n",
      "Epoch [8/10] - Loss: 0.5641 | Accuracy: 74.26%\n",
      "Epoch [9/10] - Loss: 0.4843 | Accuracy: 77.64%\n",
      "Epoch [10/10] - Loss: 0.5779 | Accuracy: 71.31%\n"
     ]
    }
   ],
   "source": [
    "fight_dataset = PoseSequenceDataset(frame_dir='results/fight_keypoints', sequence_len=30, label=1)\n",
    "nonfight_dataset = PoseSequenceDataset(frame_dir='results/nonfight_keypoints', sequence_len=30, label=0)\n",
    "\n",
    "from torch.utils.data import ConcatDataset, DataLoader\n",
    "\n",
    "combined_dataset = ConcatDataset([fight_dataset, nonfight_dataset])\n",
    "train_loader = DataLoader(combined_dataset, batch_size=8, shuffle=True)\n",
    "\n",
    "model = FightClassifierLSTM()\n",
    "train_model(model, train_loader, num_epochs=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
